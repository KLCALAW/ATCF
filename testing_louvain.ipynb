{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from modified_louvain_method import *\n",
    "from modified_spectral_method import *\n",
    "import copy\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Time Series and Covariance\n",
    "\n",
    "    XiXi​ represents the time series for node ii, and Cov[Xi,Xj]Cov[Xi​,Xj​] is the covariance between the time series XiXi​ and XjXj​.\n",
    "    When defining the renormalized interaction between two communities AA and BB, the approach should be:\n",
    "        Aggregate the time series within each community.\n",
    "        Calculate the covariance between these aggregate time series.\n",
    "\n",
    "This respects the bilinear property of covariance and ensures that the calculation captures the structure of the correlation between the communities.\n",
    "\n",
    "The earlier formula summing pairwise covariances (∑i∈A∑j∈BCov[Xi,Xj]∑i∈A​∑j∈B​Cov[Xi​,Xj​]) is mathematically equivalent to this approach because of the bilinear property of covariance. However, the \"aggregate time series and then calculate covariance\" approach provides a cleaner and more intuitive interpretation in the context of time series data.\n",
    "\n",
    "By working directly with the aggregated time series:\n",
    "\n",
    "    You treat the community as a single \"meta-node\" with its own time series.\n",
    "    This naturally extends the Louvain method to time-series-based networks.\n",
    "\n",
    "\n",
    "We therefore find that, for a graph composed of financial time series, renormalized interactions have a correct inter- pretation in terms of covariances, rather than correlations. They also show that the summation of a group of time series yields something that resembles an index fund of the set of stocks, so the concept of aggregating nodes maintains a strong grounding in reality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original covariance matrix: \n",
      "[[ 2.5   0.25  3.5   3.5 ]\n",
      " [ 0.25 14.   -5.25 -5.  ]\n",
      " [ 3.5  -5.25 13.2   4.95]\n",
      " [ 3.5  -5.    4.95  7.7 ]]\n",
      "Covariance between community 1 and 2:  -3.25\n"
     ]
    }
   ],
   "source": [
    "#Community 1\n",
    "time_series_1 = np.array([1, 2, 3, 4, 5])\n",
    "time_series_2 = np.array([5, 1, 3, 10, 1])\n",
    "community_1 = [0,1]\n",
    "\n",
    "#Community 2\n",
    "time_series_3 = np.array([1, 9, 10, 7, 9])\n",
    "time_series_4 = np.array([1, 2, 3, 2, 8])\n",
    "community_2 = [2,3]\n",
    "\n",
    "time_series_matrix = np.stack((time_series_1, time_series_2,time_series_3, time_series_4), axis=0)\n",
    "\n",
    "\n",
    "cov_matrix = np.cov(time_series_matrix) \n",
    "print(\"Original covariance matrix: \")\n",
    "print(cov_matrix)\n",
    "\n",
    "#Calculate covariance between community 1 and 2\n",
    "cov_comm1_comm2 = 0\n",
    "for i in community_1:\n",
    "    for j in community_2:\n",
    "        cov_comm1_comm2 += cov_matrix[i][j]\n",
    "    \n",
    "    \n",
    "print(\"Covariance between community 1 and 2: \", cov_comm1_comm2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17.  , -3.25],\n",
       "       [-3.25, 30.8 ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Agregate time series community 1\n",
    "time_series_1_agregate = time_series_1 + time_series_2\n",
    "\n",
    "#Agregate time series community 2\n",
    "time_series_2_agregate = time_series_3 + time_series_4\n",
    "\n",
    "\n",
    "time_series_matrix_agregate = np.stack((time_series_1_agregate, time_series_2_agregate), axis=0)\n",
    "\n",
    "cov_matrix_agregate = np.cov(time_series_matrix_agregate)\n",
    "cov_matrix_agregate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note about the final formula:\n",
    "\n",
    "Efficiency of This Calculation:\n",
    "\n",
    "    Localized Updates:\n",
    "        The formula only depends on C~IJ(l)C~IJ(l)​ and C~IJ′(l)C~IJ′(l)​, which are the total interactions between II and the hypernodes in JJ and J′J′, respectively.\n",
    "        This avoids recomputing the modularity for the entire network.\n",
    "\n",
    "    Precomputed Interactions:\n",
    "        In practice, the interactions C~IA(l)C~IA(l)​ between II and any other hypernode AA are typically precomputed, allowing C~IJ(l)C~IJ(l)​ and C~IJ′(l)C~IJ′(l)​ to be calculated quickly by summing over the relevant hypernodes in JJ and J′J′."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified Louvain Method Implementation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix,T,N,company_names = create_correlation_matrix('returns_standardized.csv')  \n",
    "C_g = calculate_C_g(correlation_matrix,T,N) #Modularity matrix\n",
    "#\n",
    "#Extract the covariance matrix of the first 4 companies (for testing purposes)\n",
    "#C_g_slice = C_g[0:4,0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0672402125997696\n",
      "-0.0672402125997696\n"
     ]
    }
   ],
   "source": [
    "#Check its symmetric!\n",
    "print(C_g[0,5])\n",
    "print(C_g[5,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adjusted for randomness\n",
    "\n",
    "#TODO: Adjust change in modularity formula --- DONE ---\n",
    "#TODO: Fix issue with map --- DONE ---\n",
    "#TODO: Adjust logic for empty communities --- DONE ---\n",
    "#TODO: Optimize algorithm\n",
    "#TODO: Hypernodes \n",
    "# TODO: Check if we need to add i<=j in the modularity calculation \n",
    "\n",
    "\n",
    "\n",
    "def calculate_modularity_with_potential_move(node,current_index,neighbour_community_index,communities, modularity_matrix):\n",
    "    \n",
    "    \"Calculate the modularity of a potential move of a node from its current community to a neighbour community\"\n",
    "\n",
    "    # Get the current and neighbor communities\n",
    "    current_community = communities[current_index]\n",
    "    neighbour_community = communities[neighbour_community_index]\n",
    "\n",
    "    #CALCULATE MODULARITY GAIN FROM ADDING NODE TO NEW COMMUNITY\n",
    "    #---------------------------------------------------\n",
    "\n",
    "    # Calculate old modularity of the neighbor community\n",
    "    # old_modularity_new_community = sum(\n",
    "    #     modularity_matrix[i][j] \n",
    "    #     for i in neighbour_community\n",
    "    #     for j in neighbour_community\n",
    "    #     if i <= j\n",
    "    # )\n",
    "\n",
    "    # Calculate new modularity after adding the node to the neighbor community\n",
    "    temp_neighbour_community = neighbour_community + [node] #We create a temp variable to avoid modifying the original community\n",
    "    new_modularity_new_community = sum(  #The new modularity can be computed by adding the modularity without the node to the additional gain after adding the node\n",
    "        modularity_matrix[node][j] #We only check pairwise correlations with the node we just added to the community\n",
    "        #for i in temp_neighbour_community\n",
    "        for j in temp_neighbour_community\n",
    "        #if i <= j\n",
    "    )\n",
    "\n",
    "    #change_from_addition = new_modularity_new_community - old_modularity_new_community\n",
    "    change_from_addition  = new_modularity_new_community\n",
    "\n",
    "    #CALCULATE MODULARITY LOSS FROM REMOVING NODE FROM CURRENT COMMUNITY\n",
    "    #---------------------------------------------------\n",
    "\n",
    "    old_modularity_current_community = sum(\n",
    "        modularity_matrix[i][j]\n",
    "        for i in current_community\n",
    "        for j in current_community\n",
    "        if i <= j\n",
    "    )\n",
    "\n",
    "    temp_current_community = copy.deepcopy(current_community) #We create a temp variable to avoid modifying the original community\n",
    "    temp_current_community.remove([node]) #Remove node from old community\n",
    "\n",
    "    new_modularity_current_communtity = sum(\n",
    "        modularity_matrix[i][j]\n",
    "        for i in temp_current_community\n",
    "        for j in temp_current_community\n",
    "        if i <= j\n",
    "    )\n",
    "\n",
    "\n",
    "    change_from_removal = old_modularity_current_community - new_modularity_current_communtity\n",
    "\n",
    "\n",
    "\n",
    "    # Normalize the modularity change\n",
    "    c_norm = np.sum(modularity_matrix)\n",
    "\n",
    "    modularity_change = (change_from_addition - change_from_removal) / c_norm\n",
    "\n",
    "    return modularity_change\n",
    "\n",
    "\n",
    "def phase_1(communities, modularity_matrix):\n",
    "\n",
    "    \n",
    "    nodes = [node for community in communities for node in community]  # Flatten all nodes into a single list\n",
    "    community_map = {node: index for index, community in enumerate(communities) for node in community}  # Map nodes to their communities\n",
    "    \n",
    "    moved = True\n",
    "    iteration = 0\n",
    "\n",
    "    while moved == True:  # Continue until no moves improve modularities\n",
    "\n",
    "        # print(f\"ITERATION {iteration}\")\n",
    "        # print(\"-----------------\")\n",
    "        \n",
    "        iteration += 1\n",
    "\n",
    "        moved = False\n",
    "        random.shuffle(nodes)  # Randomly shuffle the nodes\n",
    "        \n",
    "        for node in nodes:\n",
    "\n",
    "            #print(\"COMMUNITY MAP BELOW:\")\n",
    "            #print(community_map)\n",
    "\n",
    "            current_index = community_map[node]  # Find the node's current community index\n",
    "            #print(\"Current index\",current_index)\n",
    "            current_community = communities[current_index]\n",
    "            \n",
    "            #print(\"Node:\", node)\n",
    "            #print(\"Current Node Communty:\", current_community)\n",
    "            \n",
    "            max_modularity = 0\n",
    "            best_community = None\n",
    "            \n",
    "            for j, neighbor_community in enumerate(communities):\n",
    "\n",
    "                if not neighbor_community or neighbor_community == current_community:\n",
    "                    continue  # Skip empty or identical communities\n",
    "                \n",
    "                modularity_change = calculate_modularity_with_potential_move(node, current_index, j, communities, modularity_matrix)\n",
    "                \n",
    "                if modularity_change > max_modularity:\n",
    "                    max_modularity = modularity_change\n",
    "                    best_community = neighbor_community\n",
    "            \n",
    "            # If a modularity improvement is found, move the node\n",
    "            if max_modularity > 0 and best_community is not None:\n",
    "                current_community.remove(node)\n",
    "                best_community.append(node)\n",
    "                community_map[node] = communities.index(best_community)  # Update the community map\n",
    "                moved = True  # Indicate that a move occurred\n",
    "                #print(\"MOVED NODE\", node, \"TO COMMUNITY\", communities.index(best_community))\n",
    "\n",
    "                #print(\"UPDATED COMMUNITY MAP:\")\n",
    "                #print(community_map)\n",
    "\n",
    "        # Remove empty communities\n",
    "        communities = [community for community in communities if community]\n",
    "\n",
    "        #Update community map with new communities\n",
    "        community_map = {node: index for index, community in enumerate(communities) for node in community}  # Map nodes to their communities\n",
    "        \n",
    "        # print(f\"End of iteration {iteration}, Communities: {communities},\")\n",
    "    \n",
    "    return communities\n",
    "\n",
    "def phase_2(communities,modularity_matrix):\n",
    "\n",
    "    \"Node aggregation phase\"\n",
    "\n",
    "    # Aggregate nodes into hypernodes\n",
    "\n",
    "    #df_standardized_returns = pd.read_csv(\"returns_standardized.csv\")\n",
    "\n",
    "    renormalized_modularity_matrix = np.zeros((len(communities),len(communities))) #If there are 5 communities, this will be a 5x5 matrix\n",
    "\n",
    "    for community_index, community in enumerate(communities):\n",
    "        \n",
    "        hypernode_correlations = [] #Stores the correlation between the current community and all other communities (Including self loops)\n",
    "\n",
    "        for  neighbour_community in communities:\n",
    "            \n",
    "            # if community == neighbour_community:\n",
    "            #     continue\n",
    "            \n",
    "            # Calculate the covariance between the current community and the neighbour community\n",
    "            cov_comm1_comm2 = 0\n",
    "\n",
    "            for i in community:\n",
    "                for j in neighbour_community:\n",
    "                    cov_comm1_comm2 += modularity_matrix[i][j]\n",
    "            \n",
    "            hypernode_correlations.append(cov_comm1_comm2)\n",
    "\n",
    "        renormalized_modularity_matrix[community_index] = hypernode_correlations\n",
    "        \n",
    "\n",
    "    return renormalized_modularity_matrix\n",
    "\n",
    "def flatten_final_communities(final_communities):\n",
    "\n",
    "    flattened_communities = []\n",
    "    for item in final_communities:\n",
    "        if isinstance(item, list) and all(isinstance(subitem, list) for subitem in item):\n",
    "            # Flatten one level of nesting\n",
    "            flattened_item = [subitem for nested in item for subitem in nested]\n",
    "            flattened_communities.append(flattened_item)\n",
    "        else:\n",
    "            flattened_communities.append(item)  # Keep as is\n",
    "            \n",
    "    return flattened_communities\n",
    "\n",
    "def map_hypernodes_to_nodes(hypernode_communities, node_communities):\n",
    "\n",
    "    final_communities = []\n",
    "\n",
    "    for community in hypernode_communities:\n",
    "\n",
    "        final_community = []\n",
    "\n",
    "        for hyper_node in community:\n",
    "\n",
    "            node_community = node_communities[hyper_node]\n",
    "            final_community.append(node_community)\n",
    "            \n",
    "\n",
    "        final_communities.append(final_community)\n",
    "\n",
    "    #Flatten final communities so they are in the same shape as the original communities\n",
    "    final_communities = flatten_final_communities(final_communities)\n",
    "    \n",
    "    return final_communities\n",
    "\n",
    "\n",
    "def modified_louvain_1(modularity_matrix):\n",
    "\n",
    "    #Get node indices from matrix\n",
    "    node_indices = np.arange(modularity_matrix.shape[0])\n",
    "\n",
    "    #Create initial communities\n",
    "    initial_pahse1_communities = [[node] for node in node_indices]\n",
    "\n",
    "    #Phase 1 for nodes\n",
    "    #--------------------------------\n",
    "\n",
    "    #Calculate initial communities\n",
    "    phase1_communities = phase_1(initial_pahse1_communities, modularity_matrix)\n",
    "\n",
    "    #Phase 2 (aggregation) for hypernodes\n",
    "    #--------------------------------\n",
    "\n",
    "    renormalized_modularity_matrix = phase_2(phase1_communities,modularity_matrix)\n",
    "\n",
    "    hyper_node_indices = np.arange(renormalized_modularity_matrix.shape[0])\n",
    "\n",
    "    initial_hypernode_communities = [[node] for node in hyper_node_indices]\n",
    "\n",
    "    #print(\"Initial hypernode communities\", initial_hypernode_communities)\n",
    "\n",
    "    #Phase 1 for hypernodes\n",
    "    #--------------------------------\n",
    "\n",
    "    phase1_hypernode_communities = phase_1(initial_hypernode_communities, renormalized_modularity_matrix)\n",
    "\n",
    "    #Map back hypernode communities to node communities\n",
    "    #--------------------------------\n",
    "\n",
    "    print(\"Detected\", len(phase1_communities), \"Initially\")\n",
    "\n",
    "    final_communities = map_hypernodes_to_nodes(phase1_hypernode_communities, phase1_communities)\n",
    "\n",
    "    print(\"Final number of communities:\", len(final_communities))\n",
    "    \n",
    "    return final_communities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 5 Initially\n",
      "Final number of communities: 4\n"
     ]
    }
   ],
   "source": [
    "louvain_communities = modified_louvain_1(C_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 65, 2, 29]\n",
      "[21, 77, 33, 52, 43, 1, 53, 75, 11, 39, 64, 25, 35, 49, 41, 30, 34, 4, 20, 38, 32, 44, 28, 87, 59, 23, 84, 58, 85, 56, 10, 12, 88, 36, 80, 51, 63, 45, 24, 42]\n",
      "[31, 74, 81, 5, 62, 72, 8, 67, 89, 82, 61, 16, 17, 26, 19, 83, 18, 54, 22, 40, 13, 6, 86, 66, 14, 57, 68, 78, 73, 76, 27]\n",
      "[48, 55, 69, 7, 9, 46, 47, 79, 15, 71, 37, 0, 60, 50, 70]\n"
     ]
    }
   ],
   "source": [
    "for community in louvain_communities:\n",
    "    print(community)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 5, 6, 8, 13, 14, 16, 17, 18, 19, 22, 26, 27, 31, 33, 34, 40, 54, 57, 61, 62, 66, 67, 68, 69, 72, 73, 74, 76, 78, 81, 82, 83, 86, 89]\n",
      "[2, 3, 29, 65]\n",
      "[0, 7, 9, 37, 47, 48, 55, 60, 70, 79]\n",
      "[4, 10, 11, 12, 15, 20, 21, 23, 24, 25, 28, 30, 32, 35, 36, 38, 39, 41, 42, 43, 44, 45, 46, 49, 50, 51, 52, 53, 56, 58, 59, 63, 64, 71, 75, 77, 80, 84, 85, 87, 88]\n"
     ]
    }
   ],
   "source": [
    "#Spectral method\n",
    "spectral_communities, company_communities_spectral, modularities = recursive_spectral_method(C_g, company_names,min_size=2, modularity_threshold=0.00001)\n",
    "\n",
    "for community in spectral_communities:\n",
    "    print(community)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global modularity of modified Louvain method:  15.185476426292789\n",
      "Global modularity of spectral method:  14.87421549519166\n"
     ]
    }
   ],
   "source": [
    "def calculate_global_modularity(communities, modularity_matrix):\n",
    "\n",
    "    #Calculate the global modularity of the communities\n",
    "\n",
    "    modularity = 0\n",
    "    c_norm = np.sum(modularity_matrix)\n",
    "\n",
    "    for community in communities:\n",
    "        for i in community:\n",
    "            for j in community:  \n",
    "                if i<=j:  #Ensure that we only add each pair of nodes once! \n",
    "                    modularity += modularity_matrix[i][j]\n",
    "\n",
    "    modularity = modularity/c_norm\n",
    "\n",
    "    return modularity\n",
    "\n",
    "print(\"Global modularity of modified Louvain method: \", calculate_global_modularity(louvain_communities, C_g))\n",
    "print(\"Global modularity of spectral method: \", calculate_global_modularity(spectral_communities, C_g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_communities_louvain = []\n",
    "\n",
    "for partition in louvain_communities:\n",
    "    company_list = []\n",
    "    for i in partition:\n",
    "        company_list.append(company_names[i])\n",
    "    company_communities_louvain.append(company_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ANTO.L', 'RIO.L', 'AAL.L', 'FRES.L']\n",
      "['DCC.L', 'SPX.L', 'HL.L', 'MRO.L', 'ITRK.L', 'ADM.L', 'MNDI.L', 'SMDS.L', 'BEZ.L', 'IMI.L', 'RMV.L', 'ENT.L', 'HSX.L', 'LMP.L', 'INF.L', 'GLEN.L', 'HIK.L', 'AHT.L', 'CRDA.L', 'IHG.L', 'HLMA.L', 'JD.L', 'FRAS.L', 'WEIR.L', 'PHNX.L', 'DPLM.L', 'UTG.L', 'PSN.L', 'VTY.L', 'NXT.L', 'BDEV.L', 'BKG.L', 'WTB.L', 'HWDN.L', 'TW.L', 'MKS.L', 'RTO.L', 'KGF.L', 'EZJ.L', 'IAG.L']\n",
      "['GSK.L', 'SN.L', 'TSCO.L', 'ABF.L', 'REL.L', 'SVT.L', 'BA.L', 'SGE.L', 'WPP.L', 'ULVR.L', 'RKT.L', 'BT-A.L', 'BNZL.L', 'EXPN.L', 'CPG.L', 'UU.L', 'CNA.L', 'NG.L', 'DGE.L', 'IMB.L', 'BP.L', 'AZN.L', 'VOD.L', 'RR.L', 'BATS.L', 'PSON.L', 'SBRY.L', 'SSE.L', 'SHEL.L', 'SMIN.L', 'FCIT.L']\n",
      "['LLOY.L', 'NWG.L', 'SDR.L', 'AV.L', 'BARC.L', 'LAND.L', 'LGEN.L', 'STAN.L', 'BLND.L', 'SGRO.L', 'HSBA.L', 'III.L', 'PRU.L', 'LSEG.L', 'SMT.L']\n"
     ]
    }
   ],
   "source": [
    "for company_community in company_communities_louvain:\n",
    "    print(company_community)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADM.L', 'ABF.L', 'AZN.L', 'BA.L', 'BP.L', 'BATS.L', 'BT-A.L', 'BNZL.L', 'CNA.L', 'CPG.L', 'DGE.L', 'EXPN.L', 'FCIT.L', 'GSK.L', 'HL.L', 'HIK.L', 'IMB.L', 'NG.L', 'PSON.L', 'RKT.L', 'REL.L', 'RR.L', 'SGE.L', 'SBRY.L', 'SDR.L', 'SVT.L', 'SHEL.L', 'SN.L', 'SMIN.L', 'SSE.L', 'TSCO.L', 'ULVR.L', 'UU.L', 'VOD.L', 'WPP.L']\n",
      "['AAL.L', 'ANTO.L', 'FRES.L', 'RIO.L']\n",
      "['III.L', 'AV.L', 'BARC.L', 'HSBA.L', 'LGEN.L', 'LLOY.L', 'NWG.L', 'PRU.L', 'SMT.L', 'STAN.L']\n",
      "['AHT.L', 'BDEV.L', 'BEZ.L', 'BKG.L', 'BLND.L', 'CRDA.L', 'DCC.L', 'DPLM.L', 'EZJ.L', 'ENT.L', 'FRAS.L', 'GLEN.L', 'HLMA.L', 'HSX.L', 'HWDN.L', 'IHG.L', 'IMI.L', 'INF.L', 'IAG.L', 'ITRK.L', 'JD.L', 'KGF.L', 'LAND.L', 'LMP.L', 'LSEG.L', 'MKS.L', 'MRO.L', 'MNDI.L', 'NXT.L', 'PSN.L', 'PHNX.L', 'RTO.L', 'RMV.L', 'SGRO.L', 'SMDS.L', 'SPX.L', 'TW.L', 'UTG.L', 'VTY.L', 'WEIR.L', 'WTB.L']\n"
     ]
    }
   ],
   "source": [
    "for company_community in company_communities_spectral:\n",
    "    print(company_community)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: LISTS ARE MUTABLE OBJECTS!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "test_list = [1,2,3,4]\n",
    "\n",
    "def test_function(test_list):\n",
    "    test_list.append(5)\n",
    "    \n",
    "test_function(test_list)\n",
    "\n",
    "print(test_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [2], [3], [4]]\n",
      "[[1, 5], [2], [3], [4]]\n"
     ]
    }
   ],
   "source": [
    "original_list = [[1],[2],[3],[4]]\n",
    "\n",
    "deep_copy_list = copy.deepcopy(original_list) # Deep copy\n",
    "shallow_copy_list = original_list.copy() # Shallow copy\n",
    "\n",
    "\n",
    "#Testing\n",
    "#--------------------------\n",
    "deep_copy_list[0].append(5)\n",
    "\n",
    "print(original_list)\n",
    "print(deep_copy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "[1, 3]\n"
     ]
    }
   ],
   "source": [
    "original_list = [1, 2, 3]\n",
    "\n",
    "for item in original_list[:]:\n",
    "    print(item)\n",
    "    if item == 2:\n",
    "        original_list.remove(item)\n",
    "print(original_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shallow copies\n",
    "The use of [:] to create a copy of the list ensures that the loops iterate over the original elements of list_test even if the list is modified during the iteration. This prevents issues that can arise from modifying a list while iterating over it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1 removed\n",
      "2\n",
      "3\n",
      "4\n",
      "2\n",
      "3\n",
      "4\n",
      "2\n",
      "3\n",
      "4\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "list_test = [1,2,3,4]\n",
    "\n",
    "for i in list_test[:]:\n",
    "    for j in list_test[:]:\n",
    "        print(j)\n",
    "        if j == 1:\n",
    "            list_test.remove(j)\n",
    "            print(j, \"removed\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10804988581676901\n",
      "0.21548050242159764\n",
      "0.14085445928586848\n",
      "0.14149913944138\n",
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n"
     ]
    }
   ],
   "source": [
    "modified_louvain(C_g_slice)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Reverse list\n",
    "#--------------------------\n",
    "list_test = [1,2,3,4]\n",
    "\n",
    "for i in reversed(list_test):\n",
    "    if i == 2:\n",
    "        list_test.remove(i)\n",
    "    else:\n",
    "        print(i)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
