{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FTSE_100 = ['III.L', 'ADM.L', 'AAL.L', 'ANTO.L', 'AHT.L', 'ABF.L', 'AZN.L', 'AV.L', 'BA.L', 'BARC.L', 'BDEV.L', 'BEZ.L',\n",
    " 'BKG.L', 'BP.L', 'BATS.L', 'BLND.L', 'BT-A.L', 'BNZL.L', 'CNA.L', 'CPG.L', 'CRDA.L', 'DCC.L', 'DGE.L', 'DPLM.L', 'EZJ.L',\n",
    " 'ENT.L', 'EXPN.L', 'FCIT.L', 'FRAS.L', 'FRES.L', 'GLEN.L','GSK.L', 'HLMA.L','HL.L', 'HIK.L', 'HSX.L', 'HWDN.L', 'HSBA.L',\n",
    " 'IHG.L', 'IMI.L', 'IMB.L','INF.L', 'IAG.L', 'ITRK.L','JD.L', 'KGF.L', 'LAND.L', 'LGEN.L', 'LLOY.L', 'LMP.L', 'LSEG.L',\n",
    " 'MKS.L', 'MRO.L','MNDI.L', 'NG.L', 'NWG.L', 'NXT.L', 'PSON.L', 'PSN.L','PHNX.L', 'PRU.L', 'RKT.L', 'REL.L', 'RTO.L',\n",
    " 'RMV.L', 'RIO.L', 'RR.L', 'SGE.L', 'SBRY.L','SDR.L', 'SMT.L','SGRO.L', 'SVT.L', 'SHEL.L', 'SN.L', 'SMDS.L','SMIN.L', 'SPX.L',\n",
    " 'SSE.L', 'STAN.L', 'TW.L', 'TSCO.L', 'ULVR.L', 'UU.L', 'UTG.L', 'VTY.L','VOD.L', 'WEIR.L','WTB.L', 'WPP.L']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the S&P 500 table from the Wikipedia page\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "tables = pd.read_html(url)\n",
    "\n",
    "# Extract the first table, which contains the list of S&P 500 companies\n",
    "sp500_table = tables[0]\n",
    "\n",
    "# Display the DataFrame\n",
    "print(sp500_table)\n",
    "\n",
    "# Optionally, convert the 'Symbol' column to a Python list of tickers\n",
    "sp500_tickers = sp500_table['Symbol'].tolist()\n",
    "print(len(sp500_tickers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_date = '2001-10-01'\n",
    "end_date = '2011-09-30'\n",
    "\n",
    "df_prices = pd.DataFrame()\n",
    "df_returns = pd.DataFrame()\n",
    "df_standardized_returns = pd.DataFrame()\n",
    "counter = 0\n",
    "for ticker in FTSE100_tickers:\n",
    "    # Download data for each ticker\n",
    "    data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    data = data.rename(columns={'Adj Close':ticker})  # Rename 'Close' to 'Adj Close'\n",
    "    \n",
    "    # Calculate log returns on standardized prices\n",
    "    data['log_return'] = np.log(data[f'{ticker}'] / data[f'{ticker}'].shift(1))\n",
    "    data_return = data[['log_return']]\n",
    "    data_return = data_return.rename(columns={'log_return':ticker})\n",
    "    data_return = data_return.dropna()\n",
    "\n",
    "    data_prices = data[[ticker]]\n",
    "\n",
    "    # Standardize the returns\n",
    "    data_standardized_returns = data_return.copy(deep=True)\n",
    "    mean_return = data_return[ticker].mean()\n",
    "    std_return = data_return[ticker].std()\n",
    "    data_standardized_returns[ticker] = (data_return[ticker] - mean_return) / std_return\n",
    "    \n",
    "    if counter == 0:\n",
    "        df_prices = data_prices\n",
    "        df_returns = data_return\n",
    "        df_standardized_returns = data_standardized_returns\n",
    "    else:\n",
    "        df_prices = pd.concat([df_prices, data_prices], axis=1)\n",
    "        df_returns = pd.concat([df_returns, data_return], axis=1)\n",
    "        df_standardized_returns = pd.concat([df_standardized_returns, data_standardized_returns], axis=1)\n",
    "    counter += 1\n",
    "    print(f'{counter}/{len(sp500_tickers)}')\n",
    "df_standardized_returns.to_csv('returns_standardized_S&P.csv')  # Save the data to a CSV file\n",
    "df_prices.to_csv('stock_prices_S&P.csv')  # Save the data to a CSV file\n",
    "df_returns.to_csv('stock_returns_S&P.csv')  # Save the data to a CSV file\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_standardized_returns = pd.read_csv('returns_standardized_S&P.csv')\n",
    "df_standardized_returns = df_standardized_returns.dropna(axis=1)\n",
    "df_standardized_returns.set_index(\"Date\", inplace = True)\n",
    "df_standardized_returns.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "df_standardized_returns.to_csv('returns_standardized_S&P.csv')\n",
    "df_standardized_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = df_standardized_returns.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining the filtered correlation matrix $C^{(g)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path must be the standardized return data\n",
    "def calculate_C_g(file_path):\n",
    "    # Load the standardized returns data\n",
    "    df_standardized_returns = pd.read_csv(file_path)\n",
    "    \n",
    "    # Set the 'Date' column as the index if it's not already\n",
    "    if 'Date' in df_standardized_returns.columns:\n",
    "        df_standardized_returns.set_index('Date', inplace=True)\n",
    "    \n",
    "    # Calculate the correlation matrix on the returns data\n",
    "    correlation_matrix = df_standardized_returns.corr()\n",
    "\n",
    "    # Save the correlation matrix to a new CSV file\n",
    "    correlation_matrix.to_csv('correlation_matrix.csv')\n",
    "    \n",
    "    # Calculate the eigenvalues and eigenvectors of the correlation matrix\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(correlation_matrix)\n",
    "    \n",
    "    # Calculate lambda boundaries using RMT\n",
    "    T = len(df_standardized_returns)\n",
    "    N = len(df_standardized_returns.columns)\n",
    "    lambda_plus = (1 + np.sqrt(N / T))**2\n",
    "    lambda_min = (1 - np.sqrt(N / T))**2  # Not used in this code but calculated as per RMT\n",
    "    \n",
    "    # Obtaining eigenvalues and eigenvectors above lambda_plus\n",
    "    denoised_eigenvalues = []\n",
    "    denoised_eigenvectors = []\n",
    "    \n",
    "    for index, eigenvalue in enumerate(eigenvalues):\n",
    "        if eigenvalue > lambda_plus:\n",
    "            denoised_eigenvalues.append(eigenvalue)\n",
    "            denoised_eigenvectors.append(eigenvectors[:, index])  # Corresponding eigenvector\n",
    "    \n",
    "    # Remove the largest eigenvalue (global mode) from denoised values\n",
    "    if denoised_eigenvalues:\n",
    "        max_value = max(denoised_eigenvalues)\n",
    "        max_index = denoised_eigenvalues.index(max_value)\n",
    "        denoised_eigenvalues.pop(max_index)\n",
    "        denoised_eigenvectors.pop(max_index)\n",
    "    \n",
    "    # Reconstruct the filtered correlation matrix C^(g)\n",
    "    C_g = np.zeros_like(correlation_matrix)\n",
    "    for i, eigenvalue in enumerate(denoised_eigenvalues):\n",
    "        eigenvector = np.array(denoised_eigenvectors[i]).reshape(-1, 1)  # Column vector\n",
    "        C_g += eigenvalue * (eigenvector @ eigenvector.T)  # Outer product\n",
    "    \n",
    "    # Return the filtered correlation matrix\n",
    "    return C_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_g = calculate_C_g('returns_standardized_S&P.csv')\n",
    "C_g.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified spectral method to obtain partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_method(C_g):\n",
    "    # Perform eigendecomposition\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(C_g)\n",
    "    max_eigenvalue_index = np.argmax(eigenvalues)\n",
    "    leading_eigenvector = eigenvectors[:, max_eigenvalue_index]\n",
    "    \n",
    "    community_1 = []\n",
    "    community_2 = []\n",
    "    \n",
    "    # Creating the communities based on the sign of the eigenvector\n",
    "    for i in range(len(leading_eigenvector)):\n",
    "        if leading_eigenvector[i] > 0:\n",
    "            community_1.append(i)  \n",
    "        else:\n",
    "            community_2.append(i) \n",
    "    \n",
    "    return [community_1, community_2]\n",
    "\n",
    "def calculate_modularity(C_g, partitions):\n",
    "    # C_norm is the total sum of C_g (Eq.38)\n",
    "    C_norm = np.sum(C_g)\n",
    "    modularity = 0.0\n",
    "    \n",
    "    # Calculate modularity based on the partition\n",
    "    for community in partitions:\n",
    "        for i in community:\n",
    "            for j in community:\n",
    "                modularity += C_g[i, j]\n",
    "    \n",
    "    # Normalize modularity by C_norm\n",
    "    modularity /= C_norm\n",
    "    return modularity\n",
    "\n",
    "def recursive_spectral_method(C_g):\n",
    "    result_communities = []\n",
    "\n",
    "    # Obtain communities\n",
    "    communities = spectral_method(C_g)\n",
    "    \n",
    "    # Calculate respective modularity score\n",
    "    modularity_score = calculate_modularity(C_g, communities)\n",
    "\n",
    "def recursive_spectral_method(C_g, min_size=2, modularity_threshold=0.00001):\n",
    "    result_communities = []\n",
    "\n",
    "    # Recursive function to split communities\n",
    "    def split_community(community_nodes):\n",
    "        # If community is too small, add it directly to result_communities\n",
    "        if len(community_nodes) <= min_size:\n",
    "            result_communities.append(community_nodes)\n",
    "            return\n",
    "\n",
    "        # Extract the submatrix for the current community\n",
    "        submatrix = C_g[np.ix_(community_nodes, community_nodes)]\n",
    "\n",
    "        # Apply spectral method to split into two communities\n",
    "        communities = spectral_method(submatrix)\n",
    "\n",
    "        # Map the sub-community indices back to the original indices\n",
    "        community_1 = [community_nodes[i] for i in communities[0]]\n",
    "        community_2 = [community_nodes[i] for i in communities[1]]\n",
    "        # Calculate modularity before and after the split\n",
    "        initial_modularity = calculate_modularity(C_g, [community_nodes])\n",
    "        new_modularity = calculate_modularity(C_g, [community_1, community_2])\n",
    "        # Check if the split improves modularity significantly\n",
    "        if (new_modularity - initial_modularity) > modularity_threshold:\n",
    "            # Recursively split each resulting community\n",
    "            split_community(community_1)\n",
    "            split_community(community_2)\n",
    "        else:\n",
    "            # If modularity gain is too low, add the original community without splitting\n",
    "            result_communities.append(community_nodes)\n",
    "\n",
    "    # Start recursive splitting from the entire set of nodes\n",
    "    all_nodes = list(range(len(C_g)))\n",
    "    split_community(all_nodes)\n",
    "\n",
    "    for partition in result_communities:\n",
    "        company_list = []\n",
    "        for i in partition:\n",
    "            company_list.append(names[i+1])\n",
    "        print(company_list)\n",
    "    return result_communities\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FTSE_100 = ['III.L', 'ADM.L', 'AAL.L', 'ANTO.L', 'AHT.L', 'ABF.L', 'AZN.L', 'AV.L', 'BA.L', 'BARC.L', 'BDEV.L', 'BEZ.L',\n",
    " 'BKG.L', 'BP.L', 'BATS.L', 'BLND.L', 'BT-A.L', 'BNZL.L', 'CNA.L', 'CPG.L', 'CRDA.L', 'DCC.L', 'DGE.L', 'DPLM.L', 'EZJ.L',\n",
    " 'ENT.L', 'EXPN.L', 'FCIT.L', 'FRAS.L', 'FRES.L', 'GLEN.L','GSK.L', 'HLMA.L','HL.L', 'HIK.L', 'HSX.L', 'HWDN.L', 'HSBA.L',\n",
    " 'IHG.L', 'IMI.L', 'IMB.L','INF.L', 'IAG.L', 'ITRK.L','JD.L', 'KGF.L', 'LAND.L', 'LGEN.L', 'LLOY.L', 'LMP.L', 'LSEG.L',\n",
    " 'MKS.L', 'MRO.L','MNDI.L', 'NG.L', 'NWG.L', 'NXT.L', 'PSON.L', 'PSN.L','PHNX.L', 'PRU.L', 'RKT.L', 'REL.L', 'RTO.L',\n",
    " 'RMV.L', 'RIO.L', 'RR.L', 'SGE.L', 'SBRY.L','SDR.L', 'SMT.L','SGRO.L', 'SVT.L', 'SHEL.L', 'SN.L', 'SMDS.L','SMIN.L', 'SPX.L',\n",
    " 'SSE.L', 'STAN.L', 'TW.L', 'TSCO.L', 'ULVR.L', 'UU.L', 'UTG.L', 'VTY.L','VOD.L', 'WEIR.L','WTB.L', 'WPP.L']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recursive_spectral_method(C_g, modularity_threshold = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
